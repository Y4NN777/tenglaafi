#  Ã‰valuation du pipeline RAG du chatbot Tenglaafi

## 1. Objectif gÃ©nÃ©ral

Cette phase vise Ã  quantifier les performances globales du pipeline RAG (Retrieval-Augmented Generation) de Tenglaafi, Ã  partir d'un jeu de 20 questions mÃ©dicales portant sur les maladies tropicales et les plantes mÃ©dicinales.
Elle Ã©value la cohÃ©rence entre les documents rÃ©cupÃ©rÃ©s (R) et la rÃ©ponse gÃ©nÃ©rÃ©e (G), en suivant le protocole de l'Ã©tape 5 du hackathon :

```
collecte â†’ embeddings â†’ indexation â†’ gÃ©nÃ©ration â†’ Ã©valuation
```

## 2. Configuration expÃ©rimentale

| Ã‰lÃ©ment | DÃ©tail |
|---------|--------|
| Pipeline testÃ© | src/rag_pipeline/rag.RAGPipeline |
| Base vectorielle | ChromaDB (collection tropical_medicine) |
| ModÃ¨le d'embeddings | sentence-transformers/paraphrase-multilingual-mpnet-base-v2 |
| ModÃ¨le LLM | mistralai/Mistral-7B-Instruct-v0.3 |
| Corpus indexÃ© | data/corpus.json â€” 1531 documents |
| Questions d'Ã©valuation | evaluation/tests/evaluation_results/questions.json |
| Script d'Ã©valuation | evaluation/evaluate.py |
| Fichiers rÃ©sultats | evaluation/tests/evaluation_results/evaluation_results.json & .csv |

L'Ã©valuation s'est dÃ©roulÃ©e aprÃ¨s indexation complÃ¨te (make index), avec force_reindex=False pour utiliser la base persistÃ©e.

## 3. Structure des sorties

Le script gÃ©nÃ¨re deux fichiers complÃ©mentaires :

### evaluation_results.json

Contient pour chaque question :
- la question posÃ©e et la rÃ©ponse attendue,
- la rÃ©ponse gÃ©nÃ©rÃ©e par Tenglaafi,
- les documents sources rÃ©cupÃ©rÃ©s,
- et les mÃ©triques calculÃ©es :

```json
{
  "id": 1,
  "question": "...",
  "metrics": {
    "retrieval_precision": 0.44,
    "answer_completeness": 0.47,
    "semantic_similarity": 0.60,
    "response_time": 2.38
  }
}
```

### evaluation_results.csv

Tableau plat reprenant ces informations pour inspection manuelle, calculs externes ou visualisations.

## 4. MÃ©triques utilisÃ©es

Les mÃ©triques proviennent du module evaluation/metrics.py.

| MÃ©trique | Description | InterprÃ©tation |
|----------|-------------|----------------|
| Retrieval Precision | % de mots-clÃ©s attendus prÃ©sents dans les documents rÃ©cupÃ©rÃ©s. | CapacitÃ© du moteur vectoriel Ã  cibler les bons passages du corpus. |
| Answer Completeness | Taux de couverture des mots-clÃ©s attendus dans la rÃ©ponse. | Indique si la rÃ©ponse du LLM couvre tous les aspects essentiels. |
| Semantic Similarity | Cosine de similaritÃ© entre la rÃ©ponse gÃ©nÃ©rÃ©e et la rÃ©fÃ©rence. | Mesure la cohÃ©rence sÃ©mantique globale du texte produit. |
| Response Time (s) | Temps d'exÃ©cution moyen par requÃªte. | Indicateur de latence et d'efficacitÃ© du pipeline. |

## 5. RÃ©sultats globaux

Les rÃ©sultats agrÃ©gÃ©s sont extraits du champ "summary" du JSON, calculÃ© automatiquement par evaluate.py :

| MÃ©trique | Score moyen |
|----------|-------------|
| ðŸ”¹ PrÃ©cision retrieval moyenne | 0.4483 |
| ðŸ”¹ ComplÃ©tude rÃ©ponse moyenne | 0.4558 |
| ðŸ”¹ SimilaritÃ© sÃ©mantique moyenne | 0.607 |
| ðŸ”¹ Temps moyen de rÃ©ponse | 2.3862 s |
| ðŸ”¹ Nombre total de questions | 20 |

## 6. Analyse qualitative

###  6.1 Points forts

- **CohÃ©rence sÃ©mantique Ã©levÃ©e (0.607)** :
  le modÃ¨le Mistral-7B parvient Ã  reformuler correctement les concepts mÃ©dicaux du contexte.

- **Latence maÃ®trisÃ©e (~2.4 s)** sur CPU, remarquable pour un LLM de cette taille.

- **Robustesse du retrieval** : la prÃ©cision avoisine 45%, ce qui est bon pour un corpus de plus de 1500 documents.

###  6.2 Points Ã  amÃ©liorer

- **ComplÃ©tude moyenne faible (~0.46)** : certaines rÃ©ponses omettent des dÃ©tails prÃ©cis (symptÃ´mes secondaires, termes techniques).

- **ZÃ©ros frÃ©quents pour retrieval_precision et answer_completeness** :
  ces cas proviennent souvent de variations lexicales (pluriels, accents, synonymes).
  â†’ Un raffinage du prÃ©traitement linguistique et des mots-clÃ©s rÃ©duira ces Ã©carts.

- **Manque de citations directes** : bien que les sources soient intÃ©grÃ©es dans le contexte, le LLM ne les mentionne pas toujours explicitement dans la rÃ©ponse.

## 7. InterprÃ©tation

### 7.1 Lecture rapide des scores

| Plage | InterprÃ©tation |
|-------|----------------|
| > 0.7 | Excellente performance |
| 0.5 â€“ 0.7 | Bonne cohÃ©rence mais amÃ©liorable |
| 0.3 â€“ 0.5 | Moyenne, contextualisation partielle |
| < 0.3 | Faible ou hors-sujet |

Les rÃ©sultats actuels positionnent Tenglaafi dans la plage intermÃ©diaire supÃ©rieure (â‰ˆ0.6) :
le systÃ¨me comprend globalement les questions et rÃ©pond de faÃ§on plausible, mais l'extraction d'informations reste perfectible.

### 7.2 Impact du corpus

Les tests rÃ©vÃ¨lent que la qualitÃ© des documents indexÃ©s influence directement la prÃ©cision du retrieval.
Les textes trÃ¨s gÃ©nÃ©riques (OMS, WikipÃ©dia) rÃ©duisent la spÃ©cificitÃ© du vecteur.
Un filtrage thÃ©matique plus strict amÃ©liorerait la correspondance conceptuelle.

## 8. Perspectives d'amÃ©lioration

| Axe | Action recommandÃ©e |
|-----|-------------------|
| Retrieval | Affiner les embeddings avec un modÃ¨le biomÃ©dical francophone (BioClinicalBERT, CamemBERT-med). |
| RÃ©indexation | Filtrer les doublons et les phrases gÃ©nÃ©riques avant la vectorisation. |
| LLM | Ajouter un prompt contextuel plus directif (mention obligatoire des sources). |
| MÃ©triques | Ajouter une pondÃ©ration sur la longueur des rÃ©ponses et la diversitÃ© des sources. |
| Ã‰valuation | Introduire un ratings.json humain pour calibrer la pertinence perÃ§ue. |

## 9. Conclusion

Le pipeline RAG Tenglaafi dÃ©montre une performance solide pour une premiÃ¨re version :
- bonne comprÃ©hension contextuelle,
- cohÃ©rence sÃ©mantique stable,
- latence maÃ®trisÃ©e.

Les marges de progression se situent surtout sur la prÃ©cision du retrieval et la complÃ©tude des rÃ©ponses, deux points directement amÃ©liorables par des raffinements de corpus et de prompt.

## 10. RÃ©fÃ©rences des fichiers

| Fichier | RÃ´le | Localisation |
|---------|------|--------------|
| evaluation/evaluate.py | Script principal d'Ã©valuation | evaluation/ |
| evaluation/metrics.py | Calcul des mÃ©triques et normalisation linguistique | evaluation/ |
| evaluation/tests/evaluation_results/evaluation_results.json | RÃ©sultats complets question par question | evaluation/tests/evaluation_results/ |
| evaluation/tests/evaluation_results/evaluation_results.csv | Tableau plat pour export manuel ou Excel | evaluation/tests/evaluation_results/ |